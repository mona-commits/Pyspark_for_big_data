{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Spark_Test.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlTeUBVDL_D-",
        "colab_type": "text"
      },
      "source": [
        "# Spark DataFrames Project Exercise "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIYk95syL_EA",
        "colab_type": "text"
      },
      "source": [
        "Let's get some quick practice with your new Spark DataFrame skills, you will be asked some basic questions about some stock market data, in this case Walmart Stock from the years 2012-2017. This exercise will just ask a bunch of questions, unlike the future machine learning exercises, which will be a little looser and be in the form of \"Consulting Projects\", but more on that later!\n",
        "\n",
        "For now, just answer the questions and complete the tasks below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA3NwSV_L_EB",
        "colab_type": "text"
      },
      "source": [
        "#### Use the walmart_stock.csv file to Answer and complete the  tasks below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO3ykwq_L_EC",
        "colab_type": "text"
      },
      "source": [
        "#### Start a simple Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzmyN70uL_ED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "2afa4dfd-7538-429b-cb0b-ca7ef8fbd105"
      },
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Test').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/b0/bf9020b56492281b9c9d8aae8f44ff51e1bc91b3ef5a884385cb4e389a40/pyspark-3.0.0.tar.gz (204.7MB)\n",
            "\u001b[K     |████████████████████████████████| 204.7MB 79kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 54.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=c1bf5df6f178ae955b9416ea7bdc918943e1ffbfce3091758caf340804073fae\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/27/4d/ddacf7143f8d5b76c45c61ee2e43d9f8492fc5a8e78ebd7d37\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd01RXwmL_EO",
        "colab_type": "text"
      },
      "source": [
        "#### Load the Walmart Stock CSV File, have Spark infer the data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNsgIdIyL_EP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.read.csv('/content/walmart_stock.csv', inferSchema = True, header = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxlnB4uTL_EU",
        "colab_type": "text"
      },
      "source": [
        "#### What are the column names?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM0OpWOZL_EV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3736c66d-d6b2-462f-ad51-eace5b159481"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX-b9wuXL_Ed",
        "colab_type": "text"
      },
      "source": [
        "#### What does the Schema look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUPmKugqL_Ef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "733fa07a-e1ea-4a76-f613-d412be6b1428"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP5btymaL_Ep",
        "colab_type": "text"
      },
      "source": [
        "#### Print out the first 5 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGlOc6LL_Eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "44dc0770-88f9-408d-a808-1c68f3b3af55"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Date='2012-01-03', Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
              " Row(Date='2012-01-04', Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n",
              " Row(Date='2012-01-05', Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539),\n",
              " Row(Date='2012-01-06', Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922),\n",
              " Row(Date='2012-01-09', Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EacYJezsL_Ez",
        "colab_type": "text"
      },
      "source": [
        "#### Use describe() to learn about the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h1t7J7fL_E0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "5697f126-789c-4230-8ddc-b12e008a82fd"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYH2E3n2L_E5",
        "colab_type": "text"
      },
      "source": [
        "## Bonus Question!\n",
        "#### There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar. [Check this link for a hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast)\n",
        "\n",
        "If you get stuck on this, don't worry, just view the solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW52fxfNL_E5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "bd28e4cd-d957-4a90-d30f-684073fe9cfa"
      },
      "source": [
        "df.describe().printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- summary: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECLAHd_CL_FE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql.functions import format_number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmAZEn7_L_FN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "8c13ec95-771e-49b5-f9d9-1e59cc445813"
      },
      "source": [
        "results = df.describe()\n",
        "results.select(results['summary'], format_number(results['Open'].cast('float'),2).alias('Open'), format_number(results['Close'].cast('float'),2).alias('Close'),\n",
        "               format_number(results['High'].cast('float'),2).alias('High'), format_number(results['Low'].cast('float'),2).alias('Low'), \n",
        "               format_number(results['Volume'].cast('float'),2).alias('Volume')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------+--------+--------+--------+-------------+\n",
            "|summary|    Open|   Close|    High|     Low|       Volume|\n",
            "+-------+--------+--------+--------+--------+-------------+\n",
            "|  count|1,258.00|1,258.00|1,258.00|1,258.00|     1,258.00|\n",
            "|   mean|   72.36|   72.39|   72.84|   71.92| 8,222,093.50|\n",
            "| stddev|    6.77|    6.76|    6.77|    6.74| 4,519,781.00|\n",
            "|    min|   56.39|   56.42|   57.06|   56.30| 2,094,900.00|\n",
            "|    max|   90.80|   90.47|   90.97|   89.25|80,898,096.00|\n",
            "+-------+--------+--------+--------+--------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dChZ9oxCL_FT",
        "colab_type": "text"
      },
      "source": [
        "#### Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8twsKSkHL_FU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "outputId": "c5996dc4-0588-4b9e-cd1e-22beb701de8e"
      },
      "source": [
        "new_df = df.withColumn('HV Ratio', df['High']/df['Volume'])\n",
        "new_df.show()\n",
        "new_df.select('HV Ratio').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+--------------------+\n",
            "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|            HV Ratio|\n",
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+--------------------+\n",
            "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|4.819714653321546E-6|\n",
            "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|6.290848613094555E-6|\n",
            "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|4.669412994783916E-6|\n",
            "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|7.367338463826307E-6|\n",
            "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|8.915604778943901E-6|\n",
            "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|8.644477436914568E-6|\n",
            "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|9.351828421515645E-6|\n",
            "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994| 8.29141562102703E-6|\n",
            "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|7.712212102001476E-6|\n",
            "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|7.071764823529412E-6|\n",
            "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|1.015495466386981E-5|\n",
            "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|6.576354146362592...|\n",
            "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996| 5.90145296180676E-6|\n",
            "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|8.547679455011844E-6|\n",
            "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|8.420709512685392E-6|\n",
            "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|1.041448341728929...|\n",
            "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|8.316075414862431E-6|\n",
            "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|9.721183814992126E-6|\n",
            "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|8.029436027707578E-6|\n",
            "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|6.307432259386365E-6|\n",
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+\n",
            "|            HV Ratio|\n",
            "+--------------------+\n",
            "|4.819714653321546E-6|\n",
            "|6.290848613094555E-6|\n",
            "|4.669412994783916E-6|\n",
            "|7.367338463826307E-6|\n",
            "|8.915604778943901E-6|\n",
            "|8.644477436914568E-6|\n",
            "|9.351828421515645E-6|\n",
            "| 8.29141562102703E-6|\n",
            "|7.712212102001476E-6|\n",
            "|7.071764823529412E-6|\n",
            "|1.015495466386981E-5|\n",
            "|6.576354146362592...|\n",
            "| 5.90145296180676E-6|\n",
            "|8.547679455011844E-6|\n",
            "|8.420709512685392E-6|\n",
            "|1.041448341728929...|\n",
            "|8.316075414862431E-6|\n",
            "|9.721183814992126E-6|\n",
            "|8.029436027707578E-6|\n",
            "|6.307432259386365E-6|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AWMqNcrL_FY",
        "colab_type": "text"
      },
      "source": [
        "#### What day had the Peak High in Price?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc_941zOL_Fa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1c9c380b-6a8e-4e60-f7c3-e769e417efe0"
      },
      "source": [
        "df.createOrReplaceTempView('Analysis')\n",
        "results = spark.sql(\"Select Date from Analysis where High in (Select max(High) from Analysis)\").show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|      Date|\n",
            "+----------+\n",
            "|2015-01-13|\n",
            "+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJXp2_dVL_Fl",
        "colab_type": "text"
      },
      "source": [
        "#### What is the mean of the Close column?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGIvOyM0L_Fm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "55d38a90-8e6f-4640-cd27-7128dbfa466d"
      },
      "source": [
        "df.agg({'Close': 'Mean'}).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844998012726|\n",
            "+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28YZOFN6L_Fr",
        "colab_type": "text"
      },
      "source": [
        "#### What is the max and min of the Volume column?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z9Ym3VPL_Fs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6da81887-d446-4683-8618-a6b0a6b1ff65"
      },
      "source": [
        "df.agg({'Volume': 'Max'}).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|max(Volume)|\n",
            "+-----------+\n",
            "|   80898100|\n",
            "+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN38-fJiL_F0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c3d3321f-ec0a-4058-9fa7-1f4337c4359c"
      },
      "source": [
        "df.agg({'Volume': 'Min'}).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|min(Volume)|\n",
            "+-----------+\n",
            "|    2094900|\n",
            "+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9B300OVL_F5",
        "colab_type": "text"
      },
      "source": [
        "#### How many days was the Close lower than 60 dollars?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvIaWxqEL_F7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "48c65dcc-41ad-4f10-a0bc-eae3b19a836e"
      },
      "source": [
        "df.createOrReplaceTempView('Analysis')\n",
        "results = spark.sql(\"Select count(Date) from analysis where Close<60\")\n",
        "results.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|count(Date)|\n",
            "+-----------+\n",
            "|         81|\n",
            "+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Y7dQGvL_GA",
        "colab_type": "text"
      },
      "source": [
        "#### What percentage of the time was the High greater than 80 dollars ?\n",
        "#### In other words, (Number of Days High>80)/(Total Days in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-eS9XeEL_GB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af735c8f-011d-4a09-fa8d-e28553a65645"
      },
      "source": [
        "df.createOrReplaceTempView('Analysis')\n",
        "No_of_high = spark.sql(\"Select count(Date) from analysis where High>80\").collect()\n",
        "row = No_of_high[0]\n",
        "x = row.asDict()['count(Date)']\n",
        "# print(x)\n",
        "No_of_days = spark.sql(\"Select count(Date) from analysis\").collect()\n",
        "den = No_of_days[0]\n",
        "y = den.asDict()['count(Date)']\n",
        "# print(y)\n",
        "z = (x*100)/(y*1.0)\n",
        "print(z)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.141494435612083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bsZbxQmL_GG",
        "colab_type": "text"
      },
      "source": [
        "#### What is the Pearson correlation between High and Volume?\n",
        "#### [Hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions.corr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk3SYz_2L_GH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "df4f3f3e-465c-4883-de31-c43fac478b76"
      },
      "source": [
        "from pyspark.sql.functions import corr\n",
        "df.select(corr('High', 'Volume')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "| corr(High, Volume)|\n",
            "+-------------------+\n",
            "|-0.3384326061737161|\n",
            "+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igfe7-luL_GK",
        "colab_type": "text"
      },
      "source": [
        "#### What is the max High per year?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeAsZeD3L_GL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3730081e-ae9e-4b37-c606-72d9f47fc980"
      },
      "source": [
        "from pyspark.sql.functions import year\n",
        "ydf = df.withColumn(\"Year\", year(df['Date']))\n",
        "ydf.createOrReplaceTempView('New')\n",
        "Max_high = spark.sql(\"Select Year, Max(High) from New group by Year\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|2015|90.970001|\n",
            "|2013|81.370003|\n",
            "|2014|88.089996|\n",
            "|2012|77.599998|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ8LBpjEL_GP",
        "colab_type": "text"
      },
      "source": [
        "#### What is the average Close for each Calendar Month?\n",
        "#### In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ilmWYGL_GQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "4073d90e-2115-4b70-e015-7e9752b79f97"
      },
      "source": [
        "from pyspark.sql.functions import year, month\n",
        "ydf = df.withColumn(\"Year\", year(df['Date']))\n",
        "mydf = ydf.withColumn(\"Month\", month(df['Date']))\n",
        "mydf.createOrReplaceTempView('MYNew')\n",
        "Max_high1 = spark.sql(\"Select Year, Month, Max(High) from MYNew group by Year, Month order by Year,Month\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+------------------+\n",
            "|Year|Month|         max(High)|\n",
            "+----+-----+------------------+\n",
            "|2012|    1|              62.0|\n",
            "|2012|    2|         62.630001|\n",
            "|2012|    3|              61.5|\n",
            "|2012|    4|62.490002000000004|\n",
            "|2012|    5|         66.660004|\n",
            "|2012|    6|         69.720001|\n",
            "|2012|    7|         75.239998|\n",
            "|2012|    8|         74.959999|\n",
            "|2012|    9|         75.190002|\n",
            "|2012|   10|         77.599998|\n",
            "|2012|   11|         75.160004|\n",
            "|2012|   12|         72.699997|\n",
            "|2013|    1|         70.440002|\n",
            "|2013|    2|         71.959999|\n",
            "|2013|    3|         75.110001|\n",
            "|2013|    4|              79.5|\n",
            "|2013|    5|         79.959999|\n",
            "|2013|    6|         76.870003|\n",
            "|2013|    7|         78.690002|\n",
            "|2013|    8|              79.0|\n",
            "+----+-----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZS92ZNXL_GV",
        "colab_type": "text"
      },
      "source": [
        "# Great Job!"
      ]
    }
  ]
}